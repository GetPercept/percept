<p align="center">
  <h1 align="center">â—‰ Percept</h1>
  <p align="center"><strong>Give your AI agent ears.</strong></p>
  <p align="center"><em>Open-source ambient voice intelligence for AI agents</em></p>
</p>

<p align="center">
  <a href="#quick-start">Quick Start</a> â€¢
  <a href="docs/getting-started.md">Getting Started</a> â€¢
  <a href="docs/api-reference.md">API</a> â€¢
  <a href="docs/architecture.md">Architecture</a> â€¢
  <a href="docs/cli-reference.md">CLI</a> â€¢
  <a href="protocol/PROTOCOL.md">Protocol</a>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/license-MIT-blue.svg" alt="MIT License">
  <img src="https://img.shields.io/badge/python-3.10+-blue.svg" alt="Python 3.10+">
  <img src="https://img.shields.io/badge/OpenClaw-Compatible-green.svg" alt="OpenClaw Compatible">
  <img src="https://img.shields.io/badge/NVIDIA-Inception-76B900.svg" alt="NVIDIA Inception">
</p>

---

https://github.com/GetPercept/percept/raw/main/demo.mp4

---

Percept is an open-source ambient voice pipeline that connects wearable microphones to AI agents. Wear a pendant, speak naturally, and your agent executes voice commands, summarizes meetings, identifies speakers, and builds a searchable knowledge graph â€” all processed locally on your machine.

**What makes Percept different:** It's not just transcription. The **Context Intelligence Layer (CIL)** transforms raw speech into structured, actionable context â€” entity extraction, relationship graphs, speaker resolution, and semantic search â€” so your agent actually *understands* what's being said.

## Quick Start

```bash
# Install
pip install getpercept

# Start the server (receiver on :8900, dashboard on :8960)
percept serve

# Point your Omi webhook to:
#   https://your-host:8900/webhook/transcript
```

Say **"Hey Jarvis, remind me to check email"** and watch it work.

## âœ¨ Features

### Voice Pipeline
- ğŸ™ï¸ **Wake Word Detection** â€” "Hey Jarvis" (configurable via DB settings) triggers voice commands
- âš¡ **7 Action Types** â€” Email, text, reminders, search, calendar, notes, orders â€” by voice
- ğŸ“ **Auto Summaries** â€” Meeting summaries sent via iMessage after 60s of silence
- ğŸ—£ï¸ **Speaker Identification** â€” Say "that was Sarah" to teach it who's talking
- ğŸ‘‚ **Ambient Logging** â€” Full transcript history with timestamps and speaker labels
- ğŸ”’ **Local-First** â€” faster-whisper runs on your machine. Audio never leaves your hardware

### Context Intelligence Layer (CIL)
- ğŸ§  **Entity Extraction** â€” Two-pass pipeline: fast regex + LLM semantic extraction
- ğŸ”— **Relationship Graph** â€” Auto-builds entity relationships (mentioned_with, works_on, client_of)
- ğŸ¯ **Entity Resolution** â€” 5-tier cascade: exact â†’ fuzzy â†’ contextual â†’ recency â†’ semantic
- ğŸ” **Semantic Search** â€” NVIDIA NIM embeddings + LanceDB vector store
- ğŸ’¾ **SQLite Persistence** â€” Conversations, utterances, speakers, contacts, actions, relationships
- ğŸ“Š **FTS5 Full-Text Search** â€” Porter-stemmed search across all utterances
- â° **TTL Auto-Purge** â€” Configurable retention: utterances 30d, summaries 90d, relationships 180d

### Intent Parser
- ğŸï¸ **Two-Tier Hybrid** â€” Fast regex (handles ~80% of commands instantly) + LLM fallback
- ğŸ”¢ **Spoken Number Support** â€” "thirty minutes" â†’ 1800s, "an hour and a half" â†’ 5400s
- ğŸ“‡ **Contact Resolution** â€” "email Sarah" auto-resolves from contacts registry
- ğŸ’¬ **Spoken Email Normalization** â€” "jane at example dot com" â†’ jane@example.com

## Architecture

```
  Mic (Omi Pendant / Apple Watch)
        â”‚ BLE
  Phone App (streams audio)
        â”‚ Webhook
  Percept Receiver (FastAPI, port 8900)
   â”œâ”€ Wake word detection (from DB settings)
   â”œâ”€ Intent parser (regex + LLM)
   â”œâ”€ Conversation segmentation (3s command / 60s summary)
   â”œâ”€ Entity extraction + relationship graph
   â”œâ”€ SQLite persistence (conversations, utterances, speakers, actions)
   â”œâ”€ LanceDB vector indexing (NVIDIA NIM embeddings)
   â””â”€ Action dispatch â†’ OpenClaw / stdout / webhook
        â”‚
  Dashboard (port 8960)
   â”œâ”€ Live transcript feed
   â”œâ”€ Conversation history + search
   â”œâ”€ Analytics (words/day, speakers, actions)
   â”œâ”€ Settings management (wake words, contacts, speakers)
   â””â”€ Data export + purge
```

## Supported Hardware

| Device | Status | Notes |
|--------|--------|-------|
| **Omi Pendant** | âœ… Live | Primary device. BLE to phone, all-day battery. "Critical to our story" |
| **Apple Watch** | ğŸ”œ Beta | WatchOS app built (push-to-talk, raise-to-speak). Needs real device testing |
| **AirPods** | ğŸ”œ Planned | Via phone mic passthrough |
| **Any Webhook Source** | âœ… Ready | Standard HTTP webhook interface â€” any device that POSTs transcripts |

## Supported Actions

| Action | Voice Example | Resolution |
|--------|---------------|------------|
| **Email** | "Hey Jarvis, email Sarah about the meeting" | Contact lookup â†’ email |
| **Text** | "Hey Jarvis, text Rob I'm running late" | Contact lookup â†’ phone |
| **Reminder** | "Hey Jarvis, remind me in thirty minutes to call the dentist" | Spoken number parsing |
| **Search** | "Hey Jarvis, look up the weather in Cape Town" | Web search |
| **Note** | "Hey Jarvis, remember the API key is in the shared doc" | Context capture |
| **Calendar** | "Hey Jarvis, schedule a call with Mike tomorrow at 2pm" | Calendar integration |
| **Summary** | "Hey Jarvis, summarize this conversation" | On-demand summary |

## CLI Quick Reference

```bash
percept serve                  # Start receiver + dashboard
percept listen                 # Start receiver, output JSON events
percept status                 # Pipeline health check
percept transcripts            # List recent transcripts
percept transcripts --today    # Today's transcripts only
percept actions                # List recent voice actions
percept search "budget"        # Semantic search over conversations
percept audit                  # Data stats (conversations, utterances, storage)
percept purge --older-than 90  # Delete old data
percept config                 # Show configuration
percept config --set whisper.model_size=small
```

> See [CLI Reference](docs/cli-reference.md) for full details.

## Dashboard

The web dashboard runs on port 8960 and provides:

- **Live transcript feed** â€” real-time stream of what's being said
- **Conversation history** â€” searchable archive with speaker labels
- **Analytics** â€” words/day, segments/hour, speaker breakdown, action history
- **Settings page** â€” manage wake words, speakers, contacts, transcriber config from DB
- **Entity graph** â€” browse extracted entities and relationships
- **Search** â€” FTS5 keyword search with LanceDB vector search fallback
- **Data management** â€” export all data as JSON, purge by TTL or manually

## Transcription

| Transcriber | Status | Use Case |
|-------------|--------|----------|
| **Omi on-device** | âœ… Default | Omi app transcribes locally, sends text via webhook |
| **faster-whisper** | âœ… Built | Local transcription for raw audio (base model, int8, M-series optimized) |
| **NVIDIA Parakeet** | âœ… Tested | NVIDIA NIM ASR via gRPC. Superior accuracy, requires API key |
| **Deepgram** | ğŸ”œ Planned | Cloud ASR option |

Three-tier strategy: **Local (faster-whisper) â†’ NVIDIA (Parakeet NIM) â†’ Cloud (Deepgram)**

## Data Model (SQLite)

| Table | Purpose | Records |
|-------|---------|---------|
| `conversations` | Full conversation records with transcripts, summaries | Core |
| `utterances` | Atomic speech units (FTS5 indexed, porter stemming) | CIL atomic unit |
| `speakers` | Speaker profiles with word counts, relationships | Identity |
| `contacts` | Name â†’ email/phone lookup with aliases | Resolution |
| `actions` | Voice command history with status tracking | Audit |
| `entity_mentions` | Entity occurrences per conversation | CIL extraction |
| `relationships` | Weighted entity graph (source, target, type, evidence) | CIL knowledge |
| `settings` | Runtime config (wake words, timeouts, transcriber) | Config |

## Percept Protocol

The [Percept Protocol](protocol/PROTOCOL.md) defines a framework-agnostic JSON schema for voiceâ†’intentâ†’action handoff:

- **6 event types:** transcript, conversation, intent, action_request, action_response, summary
- **3 transports:** JSON Lines on stdout, WebSocket, Webhook
- **Unix composable:** `percept listen | jq 'select(.type == "intent")' | my-agent`

## ğŸ“– Documentation

| Doc | Description |
|-----|-------------|
| [Getting Started](docs/getting-started.md) | Install, configure Omi, first voice command |
| [Configuration](docs/configuration.md) | Config file, wake words, transcriber, CIL settings, environment variables |
| [CLI Reference](docs/cli-reference.md) | Every command, every flag, with examples |
| [API Reference](docs/api-reference.md) | Webhook endpoints, dashboard API, request/response formats |
| [Architecture](docs/architecture.md) | Pipeline diagram, CIL design, data flow, extending Percept |
| [Percept Protocol](docs/percept-protocol.md) | JSON event protocol for agent integration |
| [OpenClaw Integration](docs/openclaw-integration.md) | Using Percept with OpenClaw |
| [Decisions](docs/DECISIONS.md) | Architecture Decision Records â€” what we chose and why |
| [Roadmap](docs/ROADMAP.md) | Current status and what's next |
| [Contributing](docs/contributing.md) | Dev setup, PR guidelines, good first issues |

## Built for OpenClaw

Percept is designed as a first-class [OpenClaw](https://openclaw.ai) skill, but **works standalone** with any agent framework â€” LangChain, CrewAI, AutoGen, or a simple webhook.

```bash
# With OpenClaw
openclaw skill install percept

# Without OpenClaw â€” pipe events anywhere
percept listen --format json | your-agent-consumer
```

Five skill components: `percept-listen`, `percept-voice-cmd`, `percept-summarize`, `percept-speaker-id`, `percept-ambient`

> See [OpenClaw Integration](docs/openclaw-integration.md) for details.

## Project Structure

```
percept/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ receiver.py        # FastAPI server, webhooks, wake word, action dispatch
â”‚   â”œâ”€â”€ transcriber.py     # faster-whisper transcription, conversation tracking
â”‚   â”œâ”€â”€ intent_parser.py   # Two-tier intent parser (regex + LLM fallback)
â”‚   â”œâ”€â”€ database.py        # SQLite persistence (11 tables, FTS5, WAL mode)
â”‚   â”œâ”€â”€ context_engine.py  # CIL: Context packet assembly, entity resolution
â”‚   â”œâ”€â”€ entity_extractor.py # CIL: Two-pass entity extraction + relationship building
â”‚   â”œâ”€â”€ vector_store.py    # NVIDIA NIM embeddings + LanceDB semantic search
â”‚   â”œâ”€â”€ context.py         # Context extraction, conversation file saving
â”‚   â””â”€â”€ cli.py             # CLI entry point (9 commands)
â”œâ”€â”€ config/config.json     # Server, whisper, audio settings
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ percept.db         # SQLite database (WAL mode)
â”‚   â”œâ”€â”€ vectors/           # LanceDB vector store
â”‚   â”œâ”€â”€ conversations/     # Conversation markdown files
â”‚   â”œâ”€â”€ summaries/         # Auto-generated summaries
â”‚   â”œâ”€â”€ speakers.json      # Speaker ID â†’ name mapping
â”‚   â””â”€â”€ contacts.json      # Contact registry
â”œâ”€â”€ dashboard/
â”‚   â”œâ”€â”€ server.py          # Dashboard FastAPI backend (port 8960)
â”‚   â””â”€â”€ index.html         # Dashboard web UI
â”œâ”€â”€ protocol/
â”‚   â”œâ”€â”€ PROTOCOL.md        # Event protocol specification
â”‚   â””â”€â”€ schemas/           # JSON Schema for 6 event types
â”œâ”€â”€ landing/               # getpercept.ai landing page (port 8950)
â”œâ”€â”€ watch-app/             # Apple Watch app (push-to-talk, raise-to-speak)
â”œâ”€â”€ scripts/               # Utility scripts (backfill, vector indexing)
â”œâ”€â”€ research/              # Research notes (OpenHome, Zuna BCI, etc.)
â””â”€â”€ docs/                  # Full documentation
```

## Contributing

We'd love your help:

1. â­ **Star the repo** â€” helps more than you think
2. ğŸ§ª **Try it** â€” install, use it for a day, [file issues](https://github.com/GetPercept/percept/issues)
3. ğŸ”§ **Build** â€” language packs, hardware integrations, new action types
4. ğŸ“£ **Share** â€” blog about it, tweet about it

See [Contributing Guide](docs/contributing.md) for dev setup and PR guidelines.

## License

[MIT](LICENSE) â€” do whatever you want with it.

---

<p align="center">
  <em>"Fei-Fei Li gave AI eyes with ImageNet. We're giving AI agents ears."</em>
</p>
